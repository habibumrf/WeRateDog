{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h4> WeRateDogs Project </h4> </center>\n",
    "<center> <h4> Reporting: wragle_report </h4> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "WeRateDogs (dog_rates) is a Twitter account that rates people's dogs with a funny comment about the dog, our target is to wrangle its archive data from different sources to create a clean tidy dataframe, and then perform data analysis and visualization for the resulted dataframe. <br>\n",
    "The wrangling process includes following steps:\n",
    "<strong><ol>(1) Data Gathering</ol></strong>\n",
    "<strong><ol>(2) Data Assessing </ol></strong>\n",
    "<strong><ol>(3) Data Cleaning </ol></strong>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> <u> Data Gathering </u> </strong>\n",
    "There are three different type of dataset used in this project. <br>\n",
    "(1) <strong>Twitter_archive_enhanced.csv</strong> – The WeRateDogs Twitter archive. We manually downloaded this file.<br>\n",
    "(2) <strong>Image_predictions.tsv</strong> – The tweet image predictions, i.e., what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. This file is \n",
    "hosted on Udacity’s servers and should be downloaded programmatically using Requests library and the following URL: \n",
    "https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image\u0002predictions/image-predictions.tsv <br>\n",
    "(3) <strong>tweet_json.txt</strong> – Each tweet’s retweet count, favorite and any additional data we found interesting. Using the tweet IDs in the WERateDogs twitter archive, we could query the twitter API for each tweet’s JSON data using Python’s Tweepy library and store each tweet’s entire set of JSON data in a file called tweet_json.txt file. Each tweet’s JSON data stored in a line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Gathering: Summary\n",
    "It marks the start of the wrangling procedure. twitter_archive_enhanced.csv file was read from csv file using pandas. The requests function was used to download Image predictions.tsv. The tweet json.txt file was collected by using Tweepy to query an API and obtain a JSON object containing each tweet_ids. All of the data was then loaded into our programming environment (Jupyter Notebook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing Data\n",
    "Each piece of data was evaluated visually and programmatically for quality and tidiness issues after being collected from all relevant sources. Data type, value counts, the number of non-null entries, and duplicates are all checked during this process. Data validity, accuracy, consistency, and completeness are taken into account while evaluating quality issues. <Br>\n",
    "After assessing through visual and programmatic assessment, the following quality and tidiness issues were found <br>\n",
    "##### Quality issues\n",
    "##### In twitter_archive-enhanced-2 datatset\n",
    "- We need to drop in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id & retweeted_status_timestamp after filtering to only tweets and keep only the NaN values.\n",
    "- Extract real dog names from text and get rid of (a, an, just, ... etc) values.\n",
    "- I noticed some values in rating_denominator don't equal 10, we have to a column to represent the dog rating regardless to the denominator.\n",
    "- We can present the values of source column in more useful form.\n",
    "    \n",
    "##### Tidiness issues\n",
    "- Find a way to represent the dog stage in more useful form.\n",
    "    \n",
    "##### Quality issues\n",
    "##### image-predictions-3\n",
    "- We have less tweets than the ones in twitter-archive-enhanced-2\n",
    "###### Tidiness issues\n",
    "- Headers are values not variable names: The 9 columns of predictions can be presented in more clear form.\n",
    "    \n",
    "##### Quality issues\n",
    "##### tweet-json\n",
    "- We have less tweets than the ones in twitter-archive-enhanced-2 and much more than ones in image-preediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data\n",
    "After assessing quality and tidiness issues for all datasets, the next stage is cleaning data. In this stage we fix all quality and tidiness issues that we identified in the assessing step. Before cleaning, all datasets were copied. The process was define, code and test.\n",
    "#### archive_copy datatset\n",
    "- We dropped \"in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id & retweeted_status_timestamp\" after filtering them to only tweets and keep only NaN values in archive_copy\n",
    "- tweet_id is int instead of string so it is changed to string using .astype\n",
    "- timeatamp is object instead of datetime so it's changed to date time using pd.to_datetime function\n",
    "- Finding a way to represent the dog stage in more useful form in `archive_copy` csv file. \n",
    "==> A new column \"stage\" is create and filled in with the \"doggo\", \"puppo\". \"pupper\", \"floofer\" columns data then the four columns were dropped.\n",
    "- Rename 'p1', 'p1_conf', 'p1_dog', 'p2', 'p2_conf', 'p2_dog', 'p3', 'p3_conf', 'p3_dog' to more descriptive headers <br>\n",
    "(1) 'p1' rename to 'dog_1', 'p1_conf' rename to 'conf_1', 'p1_dog' rename to 'result_1'. <br>\n",
    "(2) 'p2' rename to 'dog_2', 'p2_conf' rename to 'conf_2', 'p2_dog' rename to 'result_2'. <br>\n",
    "(3)'p3' rename to 'dog_3', 'p3_conf' rename to 'conf_3', 'p3_dog' rename to 'result_3'.\n",
    "- Extract Tweet's link from text in `archive_copy`<br>\n",
    "==> A new column is created then Use regex to extract tweet's link from text <br>\n",
    "- Twitter for iPhone   --  1964\n",
    "- Vine - Make a Scene   --   91\n",
    "- Twitter Web Client   --    31\n",
    "- TweetDeck         --       11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness\n",
    "#### image_copy datasets\n",
    "- The dataframes archive_copy, image_copy and api_copy are merged on tweet_id named \"twitter_archive_master\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
